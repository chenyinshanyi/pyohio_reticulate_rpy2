for (j in seq_along(content_2$features)){
record = data_frame(date=content_2$features[[j]]$properties$timestamp[1],
temp=round(content_2$features[[j]]$properties$temperature[1]$value*(9/5)+32,2))
temps=bind_rows(record,temps)
}
}
date_sequence = data_frame(
start=seq.Date(from=as.Date('2019-05-01'),
to=lubridate::today()-lubridate::days(1),
by='1 day'),
end=start+lubridate::days(1)
)
station = 'KSFO'
base_url = 'https://api.weather.gov/stations/'
resource = '/observations?'
start_time = 'T00%3A56%3A00%2B00%3A00&'
end_time = 'T23%3A56%3A00%2B00%3A00'
temps = data.frame(NULL)
for (i in seq_along(date_sequence$start)){
start_date = date_sequence$start[i]
end_date = date_sequence$end[i]
url=glue(base_url,station,resource,'start=',{format(start_date,'%Y-%m-%d')},{start_time},'&end=',{format(end_date,'%Y-%m-%d')},{end_time})
page=requests$get(url)
content=page$content
content_2 = pandas_io$loads(content)
df = pandas_io$json_normalize(content_2)
for (j in seq_along(content_2$features)){
record = data_frame(date=content_2$features[[j]]$properties$timestamp[1],
temp=round(content_2$features[[j]]$properties$temperature[1]$value*(9/5)+32,2))
temps=bind_rows(record,temps)
}
}
View(temps)
View(temps)
temps$date=lubridate::parse_date_time(temps$date)
temps$date=lubridate::as.Date(temps$date)
temps$date=as.Date(temps$date)
temps$date=as.Datetime(temps$date)
date_sequence = data_frame(
start=seq.Date(from=as.Date('2019-06-14'),
to=lubridate::today()-lubridate::days(1),
by='1 day'),
end=start+lubridate::days(1)
)
station = 'KSFO'
base_url = 'https://api.weather.gov/stations/'
resource = '/observations?'
start_time = 'T00%3A56%3A00%2B00%3A00&'
end_time = 'T23%3A56%3A00%2B00%3A00'
temps = data.frame(NULL)
for (i in seq_along(date_sequence$start)){
start_date = date_sequence$start[i]
end_date = date_sequence$end[i]
url=glue(base_url,station,resource,'start=',{format(start_date,'%Y-%m-%d')},{start_time},'&end=',{format(end_date,'%Y-%m-%d')},{end_time})
page=requests$get(url)
content=page$content
content_2 = pandas_io$loads(content)
df = pandas_io$json_normalize(content_2)
for (j in seq_along(content_2$features)){
record = data_frame(date=content_2$features[[j]]$properties$timestamp[1],
temp=round(content_2$features[[j]]$properties$temperature[1]$value*(9/5)+32,2))
temps=bind_rows(record,temps)
}
}
temps$date=as.Datetime(temps$date)
temps$date=as.POSIXct(temps$date)
?
?as.POSIXct.POSIXlt
?
?as.POSIXlt
plot(temps$date,temps$temp)
temps$date
temps = data.frame(NULL)
for (i in seq_along(date_sequence$start)){
start_date = date_sequence$start[i]
end_date = date_sequence$end[i]
url=glue(base_url,station,resource,'start=',{format(start_date,'%Y-%m-%d')},{start_time},'&end=',{format(end_date,'%Y-%m-%d')},{end_time})
page=requests$get(url)
content=page$content
content_2 = pandas_io$loads(content)
df = pandas_io$json_normalize(content_2)
for (j in seq_along(content_2$features)){
record = data_frame(date=content_2$features[[j]]$properties$timestamp[1],
temp=round(content_2$features[[j]]$properties$temperature[1]$value*(9/5)+32,2))
temps=bind_rows(record,temps)
}
}
temps$date=lubridate::as_datetime(temps$date)
plot(temps$date,temps$temp)
ggplot(data=temps,x=date,y=temp) + geom_line()
ggplot(data=temps,aes(x=date,y=temp)) + geom_line()
ggplot(data=temps,aes(x=date,y=temp)) + geom_line() + geom_point()
ggplot(data=temps,aes(x=date,y=temp)) +
geom_line() +
geom_point()
date_sequence = data_frame(
start=seq.Date(from=as.Date('2019-06-14'),
to=lubridate::today()-lubridate::days(0),
by='1 day'),
end=start+lubridate::days(1)
)
station = 'KSFO'
base_url = 'https://api.weather.gov/stations/'
resource = '/observations?'
start_time = 'T00%3A56%3A00%2B00%3A00&'
end_time = 'T23%3A56%3A00%2B00%3A00'
temps = data.frame(NULL)
for (i in seq_along(date_sequence$start)){
start_date = date_sequence$start[i]
end_date = date_sequence$end[i]
url=glue(base_url,station,resource,'start=',{format(start_date,'%Y-%m-%d')},{start_time},'&end=',{format(end_date,'%Y-%m-%d')},{end_time})
page=requests$get(url)
content=page$content
content_2 = pandas_io$loads(content)
df = pandas_io$json_normalize(content_2)
for (j in seq_along(content_2$features)){
record = data_frame(date=content_2$features[[j]]$properties$timestamp[1],
temp=round(content_2$features[[j]]$properties$temperature[1]$value*(9/5)+32,2))
temps=bind_rows(record,temps)
}
}
temps$date=lubridate::as_datetime(temps$date)
?
?as.POSIXlt
ggplot(data=temps,aes(x=date,y=temp)) +
geom_line() +
geom_point()
ggplot(data=temps,aes(x=date,y=temp)) +
geom_line() +
geom_point()
temps$date=lubridate::as_datetime(temps$date,tz='America/Los Angeles')
temps$date=lubridate::as_datetime(temps$date,tz='america/los_angeles')
temps$date=lubridate::as_datetime(temps$date,tz='america/Los_Angeles')
ggplot(data=temps,aes(x=date,y=temp)) +
geom_line() +
geom_point()
ggplot(data=temps,aes(x=date,y=temp)) +
geom_line(color='gray') +
geom_point()
ggplot(data=temps,aes(x=date,y=temp)) +
geom_line(color='gray') +
geom_point()
instapy = import('instapy')
library(reticulate)
library(tidyverse)
library(glue)
pandas = import('pandas')
pandas_io = import('pandas.io.json')
requests = import('requests')
instapy = import('instapy')
use_virtualenv('instacart3')
pandas = import('pandas')
pandas_io = import('pandas.io.json')
requests = import('requests')
instapy = import('instapy')
use_virtualenv('instacart3')
instapy = import('instapy')
use_virtualenv('instacart3')
library(reticulate) ## duh :)
library(tidyverse) ## loading this for some data wrangling and data visualization at the end
library(glue) ## loading this to help with creating URLs to hit a web API
use_virtualenv('instacart3')
pandas = import('pandas')
library(reticulate) ## duh :)
library(tidyverse) ## loading this for some data wrangling and data visualization at the end
library(glue) ## loading this to help with creating URLs to hit a web API
use_virtualenv('instacart3')
pandas = import('pandas')
pandas_io = import('pandas.io.json')
requests = import('requests')
pandas$pivot
stations = read_csv('/Users/matthewbrower/git/pyohio_reticulate_rpy2/r/station_identifiers.csv')
base_url = 'https://api.weather.gov/stations/'
resource = '/observations?'
temps = data.frame(NULL)
for (i in seq_along(stations$station_identifier)){
station = stations$station_identifier[i]
url=glue(base_url,station,resource)
page=requests$get(url)
content=page$content
content_2 = pandas_io$loads(content)
df = pandas_io$json_normalize(content_2)
for (j in seq_along(content_2$features)){
record = tibble(date=content_2$features[[j]]$properties$timestamp[1],
temp=round(content_2$features[[j]]$properties$temperature[1]$value*(9/5)+32,2),
station_name=stations$name[i],
station_id = station)
temps=bind_rows(record,temps)
}
}
temps =
temps %>%
filter(date>= as.Date('2019-06-01')) %>%
mutate(date=lubridate::as_datetime(date,tz='America/Los_Angeles'))
ggplot(data=temps,aes(x=date,y=temp)) +
geom_line(color='gray') +
scale_color_gradient(low='blue',high='orange') +
geom_point(aes(color=temp)) +
facet_wrap(~station_name)
library(reticulate) ## duh :)
library(tidyverse) ## loading for data wrangling and visualization
library(glue) ## loading this to help with creating URLs to hit a web API
list(pandas)
head(stations)
stations
stations = read_csv('/Users/matthewbrower/git/pyohio_reticulate_rpy2/r/station_identifiers.csv')
stations
```{r, warning=FALSE, message=FALSE}
stations = read_csv('/Users/matthewbrower/git/pyohio_reticulate_rpy2/r/station_identifiers.csv')
library(reticulate) ## load the reticulate library
library(tidyverse) ## loading for data wrangling and visualization
library(glue) ## loading this to help with creating URLs to hit a web API
use_virtualenv('instacart3')
pandas = import('pandas')
pandas_io = import('pandas.io.json')
requests = import('requests')
pandas$pivot
stations = read_csv('/Users/matthewbrower/git/pyohio_reticulate_rpy2/r/station_identifiers.csv')
stations
base_url = 'https://api.weather.gov/stations/'
resource = '/observations?'
temps = data.frame(NULL)
for (i in seq_along(stations$station_identifier)){
station = stations$station_identifier[i]
url=glue(base_url,station,resource)
page=requests$get(url)
content=page$content
content_2 = pandas_io$loads(content)
df = pandas_io$json_normalize(content_2)
for (j in seq_along(content_2$features)){
record = tibble(date=content_2$features[[j]]$properties$timestamp[1],
temp=round(content_2$features[[j]]$properties$temperature[1]$value*(9/5)+32,2),
station_name=stations$name[i],
station_id = station)
temps=bind_rows(record,temps)
}
}
temps =
temps %>%
filter(date>= as.Date('2019-06-01')) %>%
mutate(date=lubridate::as_datetime(date,tz='America/Los_Angeles'))
ggplot(data=temps,aes(x=date,y=temp)) +
geom_line(color='gray') +
scale_color_gradient(low='blue',high='orange') +
geom_point(aes(color=temp)) +
facet_wrap(~station_name)
library(reticulate) ## load the reticulate library
library(tidyverse) ## loading for data wrangling and visualization
library(glue) ## loading this to help with creating URLs to hit a web API
use_virtualenv('instacart3')
pandas = import('pandas')
library(reticulate) ## load the reticulate library
library(tidyverse) ## loading for data wrangling and visualization
library(glue) ## loading this to help with creating URLs to hit a web API
use_virtualenv('instacart3')
pandas = import('pandas')
install.packages(c("glue", "lubridate", "reticulate", "tidyverse"))
stations = read_csv('/Users/matthewbrower/git/pyohio_reticulate_rpy2/r/reticulate_walkthrough/data/station_identifiers.csv')
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "120%",
dpi = 200
)
library(reticulate) ## load the reticulate library
library(tidyverse) ## loading for data wrangling and visualization
library(glue) ## loading this to help with creating URLs to hit a web API
use_virtualenv('instacart3')
pandas = import('pandas')
pandas_io = import('pandas.io.json')
requests = import('requests')
pandas$pivot
stations = read_csv('/Users/matthewbrower/git/pyohio_reticulate_rpy2/r/reticulate_walkthrough/data/station_identifiers.csv')
stations
base_url = 'https://api.weather.gov/stations/'
resource = '/observations?'
temps = data.frame(NULL)
for (i in seq_along(stations$station_identifier)){
station = stations$station_identifier[i]
url=glue(base_url,station,resource)
page=requests$get(url)
content=page$content
content_2 = pandas_io$loads(content)
df = pandas_io$json_normalize(content_2)
for (j in seq_along(content_2$features)){
record = tibble(date=content_2$features[[j]]$properties$timestamp[1],
temp=round(content_2$features[[j]]$properties$temperature[1]$value*(9/5)+32,2),
station_name=stations$name[i],
station_id = station)
temps=bind_rows(record,temps)
}
}
temps =
temps %>%
filter(date>= as.Date('2019-06-01')) %>%
mutate(date=lubridate::as_datetime(date,tz='America/Los_Angeles'))
ggplot(data=temps,aes(x=date,y=temp)) +
geom_line(color='gray') +
scale_color_gradient(low='blue',high='orange') +
geom_point(aes(color=temp)) +
facet_wrap(~stringr::str_wrap(station_name))
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "120%",
dpi = 200
)
ggplot(data=temps,aes(x=date,y=temp)) +
geom_line(color='gray') +
scale_color_gradient(low='blue',high='orange') +
geom_point(aes(color=temp)) +
facet_wrap(~stringr::str_wrap(station_name,25))
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "120%",
out.height = '200%',
dpi = 200
)
stations = read_csv('/Users/matthewbrower/git/pyohio_reticulate_rpy2/r/reticulate_walkthrough/data/station_identifiers.csv')
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "120%",
out.height = '200%',
dpi = 200
)
use_virtualenv('instacart3')
library(reticulate) ## load the reticulate library
library(tidyverse) ## loading for data wrangling and visualization
library(glue) ## loading this to help with creating URLs to hit a web API
use_virtualenv('instacart3')
pandas = import('pandas')
requests = import('requests')
pandas = reticulate::import('pandas')
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "120%",
out.height = '200%',
dpi = 200
)
library(reticulate) ## load the reticulate library
library(tidyverse) ## loading for data wrangling and visualization
library(glue) ## loading this to help with creating URLs to hit a web API
use_virtualenv('instacart3')
virtualenv_install('instacart3',c('pandas','requests'))
virtualenv_install('instacart3','pandas'))
virtualenv_install('instacart3','pandas')
virtualenv_install('pandas')
install.packages("reticulate")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "120%",
out.height = '200%',
dpi = 200
)
use_virtualenv()
library(reticulate) ## load the reticulate library
library(tidyverse) ## loading for data wrangling and visualization
library(glue) ## loading this to help with creating URLs to hit a web API
use_virtualenv()
virtualenv_install('pandas')
py_available()
use_python()
Sys.which
Sys.which("python")
py_discover_config()
py_discover_config()
reticulate::py_discover_config()
reticulate::py_available()
reticulate::py_available(initialize = T)
reticulate::py_discover_config()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "120%",
out.height = '200%',
dpi = 200
)
library(reticulate) ## load the reticulate library
library(tidyverse) ## loading for data wrangling and visualization
library(glue) ## loading this to help with creating URLs to hit a web API
pandas = import('pandas')
virtualenv_install('pandas')
use_virtualenv('instacart3')
virtualenv_install('pandas')
pandas = import('pandas')
requests = import('requests')
pandas_io = import('pandas.io.json')
reticulate::py_discover_config()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "120%",
out.height = '200%',
dpi = 200
)
library(reticulate) ## load the reticulate library
library(tidyverse) ## loading for data wrangling and visualization
library(glue) ## loading this to help with creating URLs to hit a web API
use_virtualenv('instacart3')
virtualenv_install('pandas')
pandas = import('pandas')
requests = import('requests')
pandas_io = import('pandas.io.json')
pandas$pivot
stations = read_csv('/Users/matthewbrower/git/pyohio_reticulate_rpy2/r/reticulate_walkthrough/data/station_identifiers.csv')
stations
base_url = 'https://api.weather.gov/stations/'
resource = '/observations?'
station = NULL
temps = data.frame(NULL)
for (i in seq_along(stations$station_identifier)){
station = stations$station_identifier[i]
url=glue(base_url,station,resource)
page=requests$get(url)
content=page$content
content_2 = pandas_io$loads(content)
df = pandas_io$json_normalize(content_2)
for (j in seq_along(content_2$features)){
record = tibble(date=content_2$features[[j]]$properties$timestamp[1],
temp=round(content_2$features[[j]]$properties$temperature[1]$value*(9/5)+32,2),
station_name=stations$short_name[i],
station_id = station)
temps=bind_rows(record,temps)
}
}
View(df)
View(record)
station = stations$station_identifier[i]
url=glue(base_url,station,resource)
page=requests$get(url)
content_2 = pandas_io$loads(content)
df = pandas_io$json_normalize(content_2)
for (j in seq_along(content_2$features)){
record = tibble(date=content_2$features[[j]]$properties$timestamp[1],
temp=round(content_2$features[[j]]$properties$temperature[1]$value*(9/5)+32,2),
station_name=stations$short_name[i],
station_id = station)
temps=bind_rows(record,temps)
}
record = tibble(date=content_2$features[[j]]$properties$timestamp[1],
temp=round(content_2$features[[j]]$properties$temperature[1]$value*(9/5)+32,2),
station_name=stations$short_name[i],
station_id = station)
pandas_io$json_normalize(content_2)
View(df)
View(df[[1]][[1]])
use_virtualenv('instacart3')
py_available()
py_config()
use_virtualenv('instacart3',required=T)
use_virtualenv('instacart3',required=T)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "120%",
out.height = '200%',
dpi = 200
)
library(reticulate) ## load the reticulate library
library(tidyverse) ## loading for data wrangling and visualization
library(glue) ## loading this to help with creating URLs to hit a web API
use_virtualenv('instacart3',required=T)
virtualenv_install('pandas')
pandas = import('pandas')
requests = import('requests')
pandas_io = import('pandas.io.json')
pandas$pivot
stations = read_csv('/Users/matthewbrower/git/pyohio_reticulate_rpy2/r/reticulate_walkthrough/data/station_identifiers.csv')
stations
base_url = 'https://api.weather.gov/stations/'
resource = '/observations?'
station = NULL
temps = data.frame(NULL)
for (i in seq_along(stations$station_identifier)){
station = stations$station_identifier[i]
url=glue(base_url,station,resource)
page=requests$get(url)
content=page$content
content_2 = pandas_io$loads(content)
df = pandas_io$json_normalize(content_2)
for (j in seq_along(content_2$features)){
record = tibble(date=content_2$features[[j]]$properties$timestamp[1],
temp=round(content_2$features[[j]]$properties$temperature[1]$value*(9/5)+32,2),
station_name=stations$short_name[i],
station_id = station)
temps=bind_rows(record,temps)
}
}
temps =
temps %>%
filter(date>= as.Date('2019-06-01')) %>%
mutate(date=lubridate::as_datetime(date,tz='America/Los_Angeles'))
ggplot(data=temps,aes(x=date,y=temp)) +
geom_line(color='gray') +
scale_color_gradient(low='blue',high='orange') +
geom_point(aes(color=temp)) +
facet_wrap(~station_name)
stations = read_csv('data/station_identifiers.csv')
View(content)
